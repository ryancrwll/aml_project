import torch
from torch.utils.data import DataLoader
import numpy as np
import os
from tqdm import tqdm
from scipy.spatial.transform import Rotation as R
import matplotlib.pyplot as plt
from sklearn.linear_model import RANSACRegressor # Unused but kept for context

# Import local modules
from dataloader import MVSECDataset
from VOnetwork import VONet

# --- Configuration (Must match train.py) ---
DATA_FILE = './data/indoor_flying1_data.hdf5'
GT_FILE = './data/indoor_flying1_gt.hdf5'
CHECKPOINT_PATH = './checkpoints/vo_model_ep50.pth' # Final checkpoint generated by the scaled training
SEQ_LEN = 5
BATCH_SIZE = 1 # Must be 1 for sequential trajectory integration
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# Model/Data Parameters
VOXEL_PARAMS = {
    'H': 260,
    'W': 346,
    'B': 5 # C: Number of channels/bins
}
OUTPUT_DIM = 6 # dx, dy, dz, d_roll, d_pitch, d_yaw

# Translation Scaling Factor (MUST MATCH TRAIN.PY)
TRANS_SCALE_FACTOR = 1000.0

# --- Multi-camera and Calibration Configuration ---
USE_STEREO = False  # If True: uses both left+right cameras (10 channels). If False: left camera only (5 channels)
USE_CALIB = False   # If True: loads calibration data from YAML file. If False: skips calibration loading
CALIB_PATH = './data/indoor_flying_calib/camchain-imucam-indoor_flying.yaml'  # Only used if USE_CALIB=True

# EXAMPLES:
#   Mono (left only):     USE_STEREO=False, USE_CALIB=False
#   Stereo fusion:        USE_STEREO=True,  USE_CALIB=False
#   Mono + Calib:         USE_STEREO=False, USE_CALIB=True
#   Stereo + Calib:       USE_STEREO=True,  USE_CALIB=True

# --- Helper Functions ---

def convert_delta_to_matrix(delta):
    """Converts a 6-DOF delta vector to a 4x4 Transformation Matrix."""
    # Use float64 for integration precision
    t = delta[:3].astype(np.float64)
    r = R.from_euler('xyz', delta[3:], degrees=False)
    R_matrix = r.as_matrix().astype(np.float64)
    T = np.eye(4, dtype=np.float64)
    T[:3, :3] = R_matrix
    T[:3, 3] = t
    return T

def compute_trajectory(pose_deltas):
    """Integrates a sequence of relative 4x4 pose deltas into an absolute trajectory."""
    trajectory = [np.eye(4, dtype=np.float64)]
    current_pose = np.eye(4, dtype=np.float64)

    for delta_T in pose_deltas:
        current_pose = current_pose @ delta_T
        trajectory.append(current_pose)

    return np.array(trajectory)

def align_trajectories_umeyama(pred_xyz, gt_xyz):
    """
    Performs optimal alignment (rotation, translation, and scale) between two
    point sets using the Umeyama algorithm (Sim(3) alignment).
    """
    N = pred_xyz.shape[0]
    if N < 3: return pred_xyz, 0.0

    # Centroids
    centroid_pred = np.mean(pred_xyz, axis=0)
    centroid_gt = np.mean(gt_xyz, axis=0)

    # Centered points
    X = pred_xyz - centroid_pred
    Y = gt_xyz - centroid_gt

    X_var_sum = np.sum(X**2)

    EPSILON = 1e-6

    if X_var_sum < EPSILON:
        # If variance is zero, alignment is meaningless; return identity transformation scale.
        return pred_xyz, 1.0

    # H matrix (Cross-covariance)
    H = X.T @ Y

    # SVD
    U, S, Vt = np.linalg.svd(H)
    V = Vt.T

    # Rotation R
    d = np.sign(np.linalg.det(V @ U.T))
    D = np.diag([1.0, 1.0, d])
    R_matrix = V @ D @ U.T

    # Scale c (Using centered points)
    c = np.trace(H @ R_matrix) / X_var_sum

    # Translation t
    t = centroid_gt - c * R_matrix @ centroid_pred

    # Apply transformation: Y_aligned = c * R_matrix @ X.T + t
    pred_aligned = (c * R_matrix @ pred_xyz.T).T + t

    return pred_aligned, c

def evaluate_model():
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"ERROR: Checkpoint not found at {CHECKPOINT_PATH}. Please train the model first.")
        return

    device = torch.device(DEVICE)

    # 1. Setup Model and Load Weights
    input_channels = VOXEL_PARAMS['B'] * 2 if USE_STEREO else VOXEL_PARAMS['B']
    model = VONet(input_channels=input_channels).to(device)
    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device))
    model.eval()
    print(f"Model loaded from {CHECKPOINT_PATH}. Stereo: {USE_STEREO}, Input channels: {input_channels}")

    # 2. Setup Data
    test_dataset = MVSECDataset(
        data_path=DATA_FILE,
        gt_path=GT_FILE,
        seq_len=SEQ_LEN,
        crop_params=VOXEL_PARAMS,
        use_stereo=USE_STEREO,
        use_calib=USE_CALIB,
        calib_path=CALIB_PATH if USE_CALIB else None
    )
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)

    all_pred_deltas = []

    # 3. Run Inference
    print("Running inference...")
    with torch.no_grad():
        for voxels, targets in tqdm(test_loader):

            if voxels.size(1) != SEQ_LEN: continue

            voxels = voxels.to(device)
            preds = model(voxels)

            preds_np = preds.cpu().numpy().reshape(-1, OUTPUT_DIM)

            # ðŸš¨ CRITICAL FIX: Scale prediction back down for accurate integration
            # Translation components (first 3) are divided by the training scale factor
            preds_np[:, :3] = preds_np[:, :3] / TRANS_SCALE_FACTOR

            all_pred_deltas.append(preds_np)

    # Concatenate all sequences
    pred_deltas = np.concatenate(all_pred_deltas, axis=0)

    # 4. GT Data Integration (Reload targets sequentially for integration)
    gt_target_deltas = []
    # We must iterate the loader again to get the ground truth targets in sequence
    test_loader_gt = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)
    for _, targets in test_loader_gt:
        gt_target_deltas.append(targets.cpu().numpy().reshape(-1, OUTPUT_DIM))
    gt_deltas = np.concatenate(gt_target_deltas, axis=0)

    # 5. Trajectory Integration
    pred_T_deltas = [convert_delta_to_matrix(d) for d in pred_deltas]
    gt_T_deltas = [convert_delta_to_matrix(d) for d in gt_deltas]

    pred_trajectory = compute_trajectory(pred_T_deltas)
    gt_trajectory = compute_trajectory(gt_T_deltas)

    # 6. Extract XYZ
    # Convert back to float32 for metric calculation
    pred_xyz_unaligned = pred_trajectory[:, :3, 3].astype(np.float32)
    gt_xyz = gt_trajectory[:, :3, 3].astype(np.float32)

    # 7. Trajectory Alignment
    print("Aligning trajectories...")
    # Pred and GT must be float64 for Umeyama, but the extraction was float32. We trust numpy handles this.
    pred_xyz_aligned, scale = align_trajectories_umeyama(pred_xyz_unaligned, gt_xyz)

    # 8. Metric Calculation
    errors_aligned = np.linalg.norm(pred_xyz_aligned - gt_xyz, axis=1)
    rmse_trans_aligned = np.sqrt(np.mean(errors_aligned**2))

    print(f"\n--- Evaluation Metrics ---")
    print(f"Total Trajectory Length (GT): {gt_xyz.shape[0]} steps")
    print(f"Prediction Scale Factor: {scale:.4f}")
    print(f"Trajectory RMSE (ALIGNED): {rmse_trans_aligned:.4f} meters")

    # 9. Test if X and Y are swapped
    print("\n--- Coordinate Swap Diagnostic ---")

    # Try swapping X and Y in predictions
    pred_xyz_swapped = pred_xyz_aligned.copy()
    pred_xyz_swapped[:, [0, 1]] = pred_xyz_swapped[:, [1, 0]]

    errors_swapped = np.linalg.norm(pred_xyz_swapped - gt_xyz, axis=1)
    rmse_swapped = np.sqrt(np.mean(errors_swapped**2))

    print(f"RMSE (X-Y original): {rmse_trans_aligned:.6f}")
    print(f"RMSE (X-Y swapped):  {rmse_swapped:.6f}")

    if rmse_swapped < rmse_trans_aligned:
        print("âš ï¸  WARNING: Predictions have X and Y swapped! Using swapped version for plot.")
        pred_xyz_aligned = pred_xyz_swapped

    # 10. Visualization
    plt.figure(figsize=(10, 8))
    plt.plot(gt_xyz[:, 0], gt_xyz[:, 1], label='Ground Truth (x, y)', color='blue')
    plt.plot(pred_xyz_aligned[:, 0], pred_xyz_aligned[:, 1], label='Predicted (ALIGNED x, y)', color='red', linestyle='--')
    plt.xlabel('X Position (m)')
    plt.ylabel('Y Position (m)')
    plt.title(f'2D Trajectory Comparison (Aligned RMSE: {rmse_trans_aligned:.4f} m)')
    plt.legend()
    plt.grid(True)
    plt.savefig('trajectory_comparison_aligned.png')
    print("Aligned trajectory plot saved to trajectory_comparison_aligned.png")

if __name__ == "__main__":
    evaluate_model()