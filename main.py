import torch
from torch.utils.data import DataLoader
import numpy as np
import os
from tqdm import tqdm
from scipy.spatial.transform import Rotation as R
import matplotlib.pyplot as plt
import argparse

# Import local modules
from dataloader import MVSECDataset
from VOnetwork import VONet

# --- Configuration (Must match train.py) ---
DATA_FILE = './data/indoor_flying1_data.hdf5'  # 1.6GB - VERY SLOW, use flying4 instead
# DATA_FILE = './data/indoor_flying4_data.hdf5'  # 671MB - FAST, recommended for testing
GT_FILE = './data/indoor_flying1_gt.hdf5'  # 3.2GB - VERY SLOW
# GT_FILE = './data/indoor_flying4_gt.hdf5'  # 707MB - FAST
CHECKPOINT_PATH = './checkpoints/vo_model_ep20.pth' # Final checkpoint generated by the scaled training
SEQ_LEN = 10
BATCH_SIZE = 1 # Must be 1 for sequential trajectory integration
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# Model/Data Parameters
VOXEL_PARAMS = {
    'H': 260,
    'W': 346,
    'B': 5 # C: Number of channels/bins CHANGE TO 10 FOR STREO USAGE
}
OUTPUT_DIM = 6 # dx, dy, dz, d_roll, d_pitch, d_yaw

# Translation Scaling Factor (MUST MATCH TRAIN.PY)
TRANS_SCALE_FACTOR = 7.5

# --- Evaluation checkpointing & chunking (helps with very large datasets) ---
ENABLE_CHECKPOINTING = True
CHUNK_SIZE = 2000        # number of sequences per checkpoint chunk
CHECKPOINT_DIR = './eval_checkpoints'
DOWNSAMPLE_ALIGN_FACTOR = 10  # use every K-th point for Umeyama alignment (speeds SVD)

# --- Multi-camera and Calibration Configuration ---
USE_STEREO = True  # If True: uses both left+right cameras (10 channels). If False: left camera only (5 channels)
USE_CALIB = True   # If True: loads calibration data from YAML file. If False: skips calibration loading
CALIB_PATH = './data/indoor_flying_calib/camchain-imucam-indoor_flying.yaml'  # Only used if USE_CALIB=True

# EXAMPLES:
#   Mono (left only):     USE_STEREO=False, USE_CALIB=False
#   Stereo fusion:        USE_STEREO=True,  USE_CALIB=False
#   Mono + Calib:         USE_STEREO=False, USE_CALIB=True
#   Stereo + Calib:       USE_STEREO=True,  USE_CALIB=True

# --- Helper Functions ---

def convert_delta_to_matrix(delta):
    """Converts a 6-DOF delta vector to a 4x4 Transformation Matrix."""
    # Use float64 for integration precision
    t = delta[:3].astype(np.float64)
    r = R.from_euler('xyz', delta[3:], degrees=False)
    R_matrix = r.as_matrix().astype(np.float64)
    T = np.eye(4, dtype=np.float64)
    T[:3, :3] = R_matrix
    T[:3, 3] = t
    return T

def compute_trajectory(pose_deltas):
    """Integrates a sequence of relative 4x4 pose deltas into an absolute trajectory."""
    trajectory = [np.eye(4, dtype=np.float64)]
    current_pose = np.eye(4, dtype=np.float64)

    for delta_T in pose_deltas:
        current_pose = current_pose @ delta_T
        trajectory.append(current_pose)

    return np.array(trajectory)

def align_trajectories_umeyama(pred_xyz, gt_xyz):
    """
    Performs optimal alignment (rotation, translation, and scale) between two
    point sets using the Umeyama algorithm (Sim(3) alignment).
    """
    N = pred_xyz.shape[0]
    if N < 3: return pred_xyz, 0.0

    # Centroids
    centroid_pred = np.mean(pred_xyz, axis=0)
    centroid_gt = np.mean(gt_xyz, axis=0)

    # Centered points
    X = pred_xyz - centroid_pred
    Y = gt_xyz - centroid_gt

    X_var_sum = np.sum(X**2)

    EPSILON = 1e-6

    if X_var_sum < EPSILON:
        # If variance is zero, alignment is meaningless; return identity transformation scale.
        return pred_xyz, 1.0

    # H matrix (Cross-covariance)
    H = X.T @ Y

    # SVD
    U, S, Vt = np.linalg.svd(H)
    V = Vt.T

    # Rotation R
    d = np.sign(np.linalg.det(V @ U.T))
    D = np.diag([1.0, 1.0, d])
    R_matrix = V @ D @ U.T

    # Scale c (Using centered points)
    c = np.trace(H @ R_matrix) / X_var_sum

    # Translation t
    t = centroid_gt - c * R_matrix @ centroid_pred

    # Apply transformation: Y_aligned = c * R_matrix @ X.T + t
    pred_aligned = (c * R_matrix @ pred_xyz.T).T + t

    # Return aligned predictions, scale, rotation matrix and translation vector
    return pred_aligned, c, R_matrix, t

def evaluate_model():
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"ERROR: Checkpoint not found at {CHECKPOINT_PATH}. Please train the model first.")
        return

    device = torch.device(DEVICE)

    # 1. Setup Model and Load Weights
    input_channels = VOXEL_PARAMS['B'] * 2 if USE_STEREO else VOXEL_PARAMS['B']
    model = VONet(input_channels=input_channels, use_stereo=USE_STEREO).to(device)
    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device))
    model.eval()
    print(f"Model loaded from {CHECKPOINT_PATH}. Stereo: {USE_STEREO}, Input channels: {input_channels}")
    print(f"Evaluating on {DATA_FILE}...")
    print(f"This may take a few minutes, depending on dataset size.")

    # 2. Setup Data
    test_dataset = MVSECDataset(
        data_path=DATA_FILE,
        gt_path=GT_FILE,
        seq_len=SEQ_LEN,
        crop_params=VOXEL_PARAMS,
        use_stereo=USE_STEREO,
        use_calib=USE_CALIB,
        calib_path=CALIB_PATH if USE_CALIB else None
    )
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)

    all_pred_deltas = []

    # 3. Run Inference
    print("Running inference...")
    # Check for existing checkpoint chunks
    if ENABLE_CHECKPOINTING and os.path.isdir(CHECKPOINT_DIR):
        chunk_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('pred_chunk_') and f.endswith('.npy')])
        if chunk_files:
            print(f"Found {len(chunk_files)} checkpoint chunk(s) in {CHECKPOINT_DIR}. Loading and resuming.")
            loaded_preds = []
            loaded_gt = []
            for f in chunk_files:
                p = np.load(os.path.join(CHECKPOINT_DIR, f))
                # corresponding gt file name
                idx = f.replace('pred_chunk_', '').replace('.npy', '')
                gname = f'gt_chunk_{idx}.npy'
                g = np.load(os.path.join(CHECKPOINT_DIR, gname))
                loaded_preds.append(p)
                loaded_gt.append(g)
            pred_deltas = np.concatenate(loaded_preds, axis=0)
            gt_deltas = np.concatenate(loaded_gt, axis=0)
        else:
            pred_deltas = None
            gt_deltas = None
    else:
        pred_deltas = None
        gt_deltas = None

    if pred_deltas is None:
        # No checkpoints found â€” run inference and save chunks
        os.makedirs(CHECKPOINT_DIR, exist_ok=True) if ENABLE_CHECKPOINTING else None
        seq_counter = 0
        chunk_idx = 0
        chunk_pred_list = []
        chunk_gt_list = []


        with torch.no_grad():
            for voxels, imu_feats, imu_ts, targets in tqdm(test_loader):

                if voxels.size(1) != SEQ_LEN: continue

                voxels = voxels.to(device)
                imu_feats = imu_feats.to(device)
                imu_ts = imu_ts.to(device)
                preds = model(voxels, imu_feats)

                preds_np = preds.cpu().numpy().reshape(-1, OUTPUT_DIM)
                targets_np = targets.cpu().numpy().reshape(-1, OUTPUT_DIM)

                # Scale prediction back down for accurate integration
                preds_np[:, :3] = preds_np[:, :3] / TRANS_SCALE_FACTOR

                # Append to current chunk lists (per-sequence chunks)
                chunk_pred_list.append(preds_np)
                chunk_gt_list.append(targets_np)
                seq_counter += 1

                # If we reached CHUNK_SIZE sequences, save chunk to disk
                if ENABLE_CHECKPOINTING and seq_counter >= CHUNK_SIZE:
                    pred_chunk = np.concatenate(chunk_pred_list, axis=0)
                    gt_chunk = np.concatenate(chunk_gt_list, axis=0)
                    np.save(os.path.join(CHECKPOINT_DIR, f'pred_chunk_{chunk_idx}.npy'), pred_chunk)
                    np.save(os.path.join(CHECKPOINT_DIR, f'gt_chunk_{chunk_idx}.npy'), gt_chunk)
                    print(f"Saved chunk {chunk_idx} ({seq_counter} sequences) to {CHECKPOINT_DIR}")
                    chunk_idx += 1
                    seq_counter = 0
                    chunk_pred_list = []
                    chunk_gt_list = []

            # Save remaining sequences in final chunk
            if chunk_pred_list:
                pred_chunk = np.concatenate(chunk_pred_list, axis=0)
                gt_chunk = np.concatenate(chunk_gt_list, axis=0)
                np.save(os.path.join(CHECKPOINT_DIR, f'pred_chunk_{chunk_idx}.npy'), pred_chunk)
                np.save(os.path.join(CHECKPOINT_DIR, f'gt_chunk_{chunk_idx}.npy'), gt_chunk)
                print(f"Saved final chunk {chunk_idx} ({len(chunk_pred_list)} sequences) to {CHECKPOINT_DIR}")

        # After inference, load all chunks to continue downstream processing
        chunk_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('pred_chunk_') and f.endswith('.npy')])
        loaded_preds = []
        loaded_gt = []
        for f in chunk_files:
            p = np.load(os.path.join(CHECKPOINT_DIR, f))
            idx = f.replace('pred_chunk_', '').replace('.npy', '')
            gname = f'gt_chunk_{idx}.npy'
            g = np.load(os.path.join(CHECKPOINT_DIR, gname))
            loaded_preds.append(p)
            loaded_gt.append(g)
        pred_deltas = np.concatenate(loaded_preds, axis=0)
        gt_deltas = np.concatenate(loaded_gt, axis=0)

    # pred_deltas and gt_deltas are loaded from checkpoint chunks or from

    # 5. Trajectory Integration
    pred_T_deltas = [convert_delta_to_matrix(d) for d in pred_deltas]
    gt_T_deltas = [convert_delta_to_matrix(d) for d in gt_deltas]

    pred_trajectory = compute_trajectory(pred_T_deltas)
    gt_trajectory = compute_trajectory(gt_T_deltas)

    # 6. Extract XYZ
    # Convert back to float32 for metric calculation
    pred_xyz_unaligned = pred_trajectory[:, :3, 3].astype(np.float32)
    gt_xyz = gt_trajectory[:, :3, 3].astype(np.float32)

    # 7. Trajectory Alignment
    align = True
    if align:
        print("Aligning trajectories...")
        # Use downsampled points to compute Umeyama for speed on very large trajectories
        N = pred_xyz_unaligned.shape[0]
        if DOWNSAMPLE_ALIGN_FACTOR > 1 and N > DOWNSAMPLE_ALIGN_FACTOR:
            idxs = np.arange(0, N, DOWNSAMPLE_ALIGN_FACTOR)
            pred_down = pred_xyz_unaligned[idxs].astype(np.float64)
            gt_down = gt_xyz[idxs].astype(np.float64)
            pred_aligned_down, scale, R_matrix, t = align_trajectories_umeyama(pred_down, gt_down)
            # Apply transform to full prediction set
            pred_xyz_aligned = (scale * R_matrix @ pred_xyz_unaligned.T).T + t
        else:
            pred_xyz_aligned, scale, R_matrix, t = align_trajectories_umeyama(pred_xyz_unaligned.astype(np.float64), gt_xyz.astype(np.float64))
            pred_xyz_aligned = pred_xyz_aligned.astype(np.float32)
    else:
        pred_xyz_aligned = pred_xyz_unaligned
        scale =1.0

    # 8. Metric Calculation
    errors_aligned = np.linalg.norm(pred_xyz_aligned - gt_xyz, axis=1)
    rmse_trans_aligned = np.sqrt(np.mean(errors_aligned**2))

    print(f"\n--- Evaluation Metrics ---")
    print(f"Total Trajectory Length (GT): {gt_xyz.shape[0]} steps")
    print(f"Prediction Scale Factor: {scale:.4f}")
    print(f"Trajectory RMSE (ALIGNED): {rmse_trans_aligned:.4f} meters")

    # 9. Visualization
    plt.figure(figsize=(10, 8))
    plt.plot(gt_xyz[:, 0], gt_xyz[:, 1], label='Ground Truth (x, y)', color='blue')
    plt.plot(pred_xyz_aligned[:, 0], pred_xyz_aligned[:, 1], label='Predicted (ALIGNED x, y)', color='red', linestyle='--')
    plt.xlabel('X Position (m)')
    plt.ylabel('Y Position (m)')
    plt.title(f'2D Trajectory Comparison (Aligned RMSE: {rmse_trans_aligned:.4f} m)')
    plt.legend()
    plt.grid(True)
    plt.savefig('trajectory_comparison_aligned.png')
    print("Aligned trajectory plot saved to trajectory_comparison_aligned.png")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Evaluate VO model with checkpointed chunking and resume support')
    parser.add_argument('--data-file', default=DATA_FILE, help='Path to event data HDF5')
    parser.add_argument('--gt-file', default=GT_FILE, help='Path to ground-truth HDF5')
    parser.add_argument('--checkpoint', default=CHECKPOINT_PATH, help='Model checkpoint path')
    parser.add_argument('--seq-len', type=int, default=SEQ_LEN, help='Sequence length')
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE, help='Batch size for DataLoader')
    parser.add_argument('--num-workers', type=int, default=4, help='DataLoader num_workers')
    parser.add_argument('--enable-checkpointing', action='store_true', dest='enable_checkpointing', help='Enable evaluation checkpointing')
    parser.add_argument('--no-checkpointing', action='store_false', dest='enable_checkpointing', help='Disable evaluation checkpointing')
    parser.set_defaults(enable_checkpointing=ENABLE_CHECKPOINTING)
    parser.add_argument('--chunk-size', type=int, default=CHUNK_SIZE, help='Number of sequences per checkpoint chunk')
    parser.add_argument('--checkpoint-dir', default=CHECKPOINT_DIR, help='Directory to store evaluation chunks')
    parser.add_argument('--downsample', type=int, default=DOWNSAMPLE_ALIGN_FACTOR, help='Downsample factor for alignment')
    parser.add_argument('--use-stereo', action='store_true', dest='use_stereo', help='Use stereo input')
    parser.add_argument('--no-stereo', action='store_false', dest='use_stereo', help='Use mono input')
    parser.set_defaults(use_stereo=USE_STEREO)
    parser.add_argument('--use-calib', action='store_true', dest='use_calib', help='Apply calibration undistortion')
    parser.add_argument('--no-calib', action='store_false', dest='use_calib', help='Do not apply calibration')
    parser.set_defaults(use_calib=USE_CALIB)
    parser.add_argument('--resume', action='store_true', help='Resume from existing evaluation chunks if present')
    parser.add_argument('--force', action='store_true', help='Force restart: delete existing chunks and start fresh')
    parser.add_argument('--run', action='store_true', help='Immediately start evaluation with provided args')

    args = parser.parse_args()

    # Apply CLI args to module-level config used by evaluate_model
    DATA_FILE = args.data_file
    GT_FILE = args.gt_file
    CHECKPOINT_PATH = args.checkpoint
    SEQ_LEN = args.seq_len
    BATCH_SIZE = args.batch_size
    CHUNK_SIZE = args.chunk_size
    CHECKPOINT_DIR = args.checkpoint_dir
    ENABLE_CHECKPOINTING = bool(args.enable_checkpointing)
    DOWNSAMPLE_ALIGN_FACTOR = args.downsample
    USE_STEREO = bool(args.use_stereo)
    USE_CALIB = bool(args.use_calib)

    # Handle force restart
    if args.force and os.path.isdir(CHECKPOINT_DIR):
        import shutil
        print(f"Force: deleting existing checkpoint dir {CHECKPOINT_DIR}")
        shutil.rmtree(CHECKPOINT_DIR)

    # If resume requested but no checkpoint dir exists, warn and continue
    if args.resume and not os.path.isdir(CHECKPOINT_DIR):
        print(f"Resume requested but no checkpoint dir found at {CHECKPOINT_DIR}. Running full inference.")

    # Start evaluation immediately if requested
    if args.run:
        evaluate_model()
    else:
        print("CLI parsed. Use --run to start evaluation now. Current config:")
        print(f"  data: {DATA_FILE}\n  gt: {GT_FILE}\n  checkpoint: {CHECKPOINT_PATH}\n  chunk_dir: {CHECKPOINT_DIR}\n  chunk_size: {CHUNK_SIZE}\n  enable_checkpointing: {ENABLE_CHECKPOINTING}\n  downsample: {DOWNSAMPLE_ALIGN_FACTOR}")